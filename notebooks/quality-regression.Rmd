---
title: "Linear Regression"
author: "Jose Luis Contreras Santos, Antonio Javier González Ferrer, Alejandro González Pérez"
header-includes:
  - \usepackage{bm}
output:
  pdf_document: 
    citation_package: natbib
    keep_tex: true
    fig_caption: true
abstract: An attempt to use linear regression to predict wine quality.
keywords: "pandoc, r markdown, knitr"
date: "`r format(Sys.time(), '%B, %Y')`"
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tseries)
library(lmtest)

# Setting working directory
# Jose
#setwd("/home/jose/repos/wines-data-analysis/src")

df <- read_delim("../data/processed/wines.csv", ";", escape_double = FALSE, trim_ws = TRUE)

# Type is a categorical variables.
df$type <- as.factor(df$type)
```

# Introduction

The aim of this section is to use linear regression to model and predict a wine's quality based on its physicochemical attributes. To do this, we will consider the variable `quality` as a continuous variable ranging from 0 to 10. It is worth noting, however, that this variable is originally a categorical one which can only take one of the 11 integer values comprised between 0 and 10. Therefore, as predictions will be continuous, many of them will most likely be slightly off due to this discrepancy. This should not pose a problem,  as the usual error metrics, such as RMSE, can be obtained anyways.

# Model fitting

After splitting the data in training and test sets, the training set was used to fit a linear regression model. At first, all variables were used as explanatory variables for the model. 

```{r first_model, message=FALSE, warning=FALSE}
# Train and test dataset, split 80% (data has been shuffled previously, no need to sample randomly)
split = nrow(df)*0.8
train = df[1:split,]
test = df[split:nrow(df),]

# Fit the model using all variables
model1 = lm(quality~fixed_acidity+volatile_acidity+citic_acid
            +residual_sugar+chlorides+free_sulfur_dioxide
            +total_sulfur_dioxide+density+pH+sulphates+alcohol, data=train)
summary(model1)
```

  However, as the summary above indicates, the p-values for citic acid and chlorides are not low enough to reject the null hypothesis. Thus, we can consider that the contribution of these to the variance of quality is not significantly greater than 0, and so they have been removed from the final model. The resulting linear regression model is described below.

```{r refine_model, message=FALSE, warning=FALSE}
# Citic_acid removed from the model
model1 = update(model1, ~.-citic_acid)
# Check if chlorides can also be removed
summary(model1)

# It can indeed be removed, do it
model1 = update(model1, ~.-chlorides)
summary(model1)
```

According to the obtained model, the two variables with the highest influence on quality are `volatile acidity` and `density`, specially the latter. This can be misleading however, as it represents the variation in quality per unit of the input, and the scales of the input variables differ in their order of magnitude. Checking the distribution of $density$, for example, the difference between the maximum and minimun values is lower than 0.03, whereas $total_sulfur_dioxide$ has a range of over 400. In any case, some conclusions can be extracted from this output. For example, denser wines tend to have a smaller perceived quality due to the negative coefficient, and, on the contrary, those with higher alcohol contents obtain highger quality perception.

The adjusted R-squared coefficient of the resulting model is rather low, with a value of only 0.29. The regression overall $p$-value, however, is low enough for us to be confident that there exists a relationship between at least part of the input and the output variable, $quality$, performing better than just the simple constant model.

# Assessing the model: assumptions 

Let us further examine the quality of the linear regression model by checking if the assumptions are met. In particular, we are interested in testing if the residuals are independent, normal and have constant variance.


```{r assumptions, message=FALSE, warning=FALSE}
# A: Check the assumptions
# Linearity
raintest(model1) # Yes, linear

# Test normality
jarque.bera.test(residuals(model1)) # Answer: No normality

# Equal variances
bptest(model1) # No constant variance

# Testing independence of the residuals
dwtest(model1, alternative="two.sided") # Not independent
```

The conducted tests have different results. First of all, the Rainbow test tests the linear relationship between the response and the linear predictor. The $p$-value is high enough to not reject the null hypothesis (0.4325) but in other datasets this value is close to 1. On the other hand, the Durbin-Watson test returns a high $p$-value (0.7377), sign that there is not enough evidence to reject the null hypothesis of autocorrelation of the residuals. Hence, the residuals can be considered as independent. The results for the Jarque-Bera and Breusch-Pagan tests, however, are not so positive. According to their results, residuals are neither normal nor homoscedastic.

```{r residualplot, fig.align="center", fig.width=6, fig.height=5, echo=FALSE, fig.cap="Residual plot", fig.retina=1}
# Residuals
par(mfrow=c(2,2))
plot(model1, which=c(1:4), ask=F)
```

Although the residual plots are unusual due to the clustering around the integer values, they confirm the previous results. The Normal Q-Q plot is in line with the results of the Jarque-Bera test, showing departures from normality, specially in the first quartiles. From the Cook's distance plot, a significant outlier can be seen, corresponding to observation 4446. Even if we remove this outlier, the statistical assumptions remain the same.

# Evaluation of results

The final step is to evaluate the performance of the model. In order to do so, let us measure the accuracy of predictions by calculating the value of our error metric of choice: RMSE. The standard deviation of our data will be used as a reference to assess the correctness of RMSE values.

```{r evaluation, message=FALSE, warning=FALSE}
# Standard deviation of our data
sd(df$quality)

# 2. Assess the model. B: Evaluate performance
predicted <- predict(model1, test)
rmse <- (sqrt(mean((test$quality - predicted)^2)))
rmse
```

As the results above show, our predicted variable $quality$ has a standard deviation of $\sigma = 0.87$. When fed with the test set, the linear regression model outputs predictions with an $RMSE = 0.74$. Therefore, and in line with what has been stated before, the interpretation of these results is positive, as RMSE is lower than the data's standard deviation.  

```{r evaluation_2, message=FALSE, warning=FALSE, include=FALSE}
# Percentage of correct predictions aka accuracy (we are rounding the predicted variable)
# Around 50% are correct - not bad
sum(test$quality == round(predicted))/nrow(test)

# Pseudo rmse (after discretizing the output var by rounding)
# Aumenta bastante, pero ahora - yo diria - es una cifra mas indicativa
rmse.discrete <- (sqrt(mean((test$quality - round(predicted))^2)))
rmse.discrete
```

## Alternative model based on the sweetness of wines

We can try to improve the validation and interpretation of the model by defininig a new categorical variable, $residual_sugar2$, which divides wines in three groups according on their sweetness[1]. The sweetness of the wines is defined by its residual sugar, and commonly the classification is `dry` wines for values up to 4 g/l, `medium_dry` up to 12 g/l,  `medium` up to 45 g/l and `sweet` more than 45 g/l. In out dataset, the highest value for `residual_sugar` is 18, thus we only consider the first three clusters.

```{r sweetness}
# Adding the new attribute
df$residual_sugar2 <- df$residual_sugar
df$residual_sugar2[df$residual_sugar < 4.0] <- "dry"
df$residual_sugar2[df$residual_sugar >= 4.0 & df$residual_sugar < 12.0] <- "medium dry"
df$residual_sugar2[df$residual_sugar >= 12.0] <- "medium"

df$residual_sugar2 = as.factor(df$residual_sugar2)
df$residual_sugar2=relevel(df$residual_sugar2, ref="dry")

# Update train and test sets 
train = df[1:split,]
test = df[split:nrow(df),]

model2 = update(model1, ~.+residual_sugar2+residual_sugar:residual_sugar2)
summary(model2)

# Test linearity
raintest(model2) # Yes, linear

# Test normality
jarque.bera.test(residuals(model2)) # Answer: No normality

# Equal variances
bptest(model2) # No constant variance

# Testing independence of the residuals
dwtest(model2, alternative="two.sided") # Not independent

# 2. Assess the model. B: Evaluate performance
predicted2 <- predict(model2, test)
rmse <- (sqrt(mean((test$quality - predicted2)^2)))
rmse
```

In comparison with the previous model, the R-squared coeffient has improved slightly. The model's assumptions, however, remain the same: residuals are linear, but they are not normal and their variance is still not constant either. RMSE does not change significantly, differing in less than 0.01 from the previous measure. 

Overall, the improvement in R-squared does not justify the effort to include a new attribute, as a) it is very slight, b) the model's assumptions are still not met, and c) RMSE has not improved as a consequence of it. It is true, however, that by looking at the coefficients, some information can be derived about the impact of sweetness on quality. Specifically, medium wines seem to have higher quality rankings than medium-dry ones, which in turn are ranked better than dry wines. 

# Conclusions 

According to evaluation results, the model has a rather good performance at predicting wine quality. Even though the it does not have a very high R-squared coefficient, the RMSE metric is low enough for us to consider it an interesting option to predict a wine's quality. Some remarks can be made, however. First of all, our input data is not uniformly distributed, with a majority of the observations taking quality values between 4 and 6. Thus, our model could be slightly overfitted for those kind of values, and there is a risk that the precision of results could drop if the input data happened to contain more extreme values. 

Besides, it would be interesting to try a different approach to this problem, perhaps using a classification technique for prediction instead, although linear regression seems better suited given the features of the output variable. 


# Bibliography

[1] Terms used to indicate sweetness of wine. https://en.wikipedia.org/wiki/Sweetness_of_wine#Residual_sugar
