\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Linear Regression},
            pdfauthor={Jose Luis Contreras Santos, Antonio Javier González Ferrer, Alejandro González Pérez},
            pdfkeywords={pandoc, r markdown, knitr},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Linear Regression}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Jose Luis Contreras Santos, Antonio Javier González Ferrer, Alejandro
González Pérez}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{enero, 2017}

\usepackage{bm}

\begin{document}
\maketitle
\begin{abstract}
An attempt to use linear regression to predict wine quality.
\end{abstract}

\section{Introduction}\label{introduction}

The aim of this section is to use linear regression to model and predict
a wine's quality based on its physicochemical attributes. To do this, we
will consider the variable \texttt{quality} as a continuous variable
ranging from 0 to 10. It is worth noting, however, that this variable is
originally a categorical one which can only take one of the 11 integer
values comprised between 0 and 10. Therefore, as predictions will be
continuous, many of them will most likely be slightly off due to this
discrepancy. This should not pose a problem, as the usual error metrics,
such as RMSE, can be obtained anyways.

\section{Model fitting}\label{model-fitting}

After splitting the data in training and test sets, the training set was
used to fit a linear regression model. At first, all variables were used
as explanatory variables for the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Train and test dataset, split 80% (data has been shuffled previously, no need to sample randomly)}
\NormalTok{split =}\StringTok{ }\KeywordTok{nrow}\NormalTok{(df)*}\FloatTok{0.8}
\NormalTok{train =}\StringTok{ }\NormalTok{df[}\DecValTok{1}\NormalTok{:split,]}
\NormalTok{test =}\StringTok{ }\NormalTok{df[split:}\KeywordTok{nrow}\NormalTok{(df),]}

\CommentTok{# Fit the model using all variables}
\NormalTok{model1 =}\StringTok{ }\KeywordTok{lm}\NormalTok{(quality~fixed_acidity+volatile_acidity+citic_acid}
            \NormalTok{+residual_sugar+chlorides+free_sulfur_dioxide}
            \NormalTok{+total_sulfur_dioxide+density+pH+sulphates+alcohol, }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = quality ~ fixed_acidity + volatile_acidity + citic_acid + 
##     residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + 
##     density + pH + sulphates + alcohol, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8027 -0.4544 -0.0414  0.4593  2.7242 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)           5.556e+01  1.399e+01   3.973 7.21e-05 ***
## fixed_acidity         6.376e-02  1.793e-02   3.556 0.000380 ***
## volatile_acidity     -1.304e+00  8.613e-02 -15.136  < 2e-16 ***
## citic_acid           -1.199e-01  8.904e-02  -1.347 0.178173    
## residual_sugar        4.162e-02  5.878e-03   7.080 1.64e-12 ***
## chlorides            -4.269e-01  3.602e-01  -1.185 0.236015    
## free_sulfur_dioxide   6.192e-03  8.400e-04   7.371 1.96e-13 ***
## total_sulfur_dioxide -2.547e-03  3.093e-04  -8.236 2.23e-16 ***
## density              -5.481e+01  1.427e+01  -3.842 0.000124 ***
## pH                    4.673e-01  1.028e-01   4.546 5.59e-06 ***
## sulphates             7.452e-01  8.423e-02   8.847  < 2e-16 ***
## alcohol               2.663e-01  1.957e-02  13.609  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.7338 on 5184 degrees of freedom
## Multiple R-squared:  0.2918, Adjusted R-squared:  0.2903 
## F-statistic: 194.1 on 11 and 5184 DF,  p-value: < 2.2e-16
\end{verbatim}

However, as the summary above indicates, the p-values for citic acid and
chlorides are not low enough to reject the null hypothesis. Thus, we can
consider that the contribution of these to the variance of quality is
not significantly greater than 0, and so they have been removed from the
final model. The resulting linear regression model is described below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Citic_acid removed from the model}
\NormalTok{model1 =}\StringTok{ }\KeywordTok{update}\NormalTok{(model1, ~.-citic_acid)}
\CommentTok{# Check if chlorides can also be removed}
\KeywordTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = quality ~ fixed_acidity + volatile_acidity + residual_sugar + 
##     chlorides + free_sulfur_dioxide + total_sulfur_dioxide + 
##     density + pH + sulphates + alcohol, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7891 -0.4580 -0.0373  0.4598  2.7323 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)           5.554e+01  1.399e+01   3.971 7.26e-05 ***
## fixed_acidity         5.809e-02  1.743e-02   3.333 0.000866 ***
## volatile_acidity     -1.259e+00  7.940e-02 -15.852  < 2e-16 ***
## residual_sugar        4.136e-02  5.876e-03   7.038 2.20e-12 ***
## chlorides            -5.103e-01  3.549e-01  -1.438 0.150475    
## free_sulfur_dioxide   6.232e-03  8.395e-04   7.424 1.33e-13 ***
## total_sulfur_dioxide -2.614e-03  3.053e-04  -8.563  < 2e-16 ***
## density              -5.479e+01  1.427e+01  -3.840 0.000124 ***
## pH                    4.770e-01  1.025e-01   4.652 3.38e-06 ***
## sulphates             7.409e-01  8.417e-02   8.801  < 2e-16 ***
## alcohol               2.642e-01  1.951e-02  13.544  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.7339 on 5185 degrees of freedom
## Multiple R-squared:  0.2915, Adjusted R-squared:  0.2901 
## F-statistic: 213.3 on 10 and 5185 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# It can indeed be removed, do it}
\NormalTok{model1 =}\StringTok{ }\KeywordTok{update}\NormalTok{(model1, ~.-chlorides)}
\KeywordTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = quality ~ fixed_acidity + volatile_acidity + residual_sugar + 
##     free_sulfur_dioxide + total_sulfur_dioxide + density + pH + 
##     sulphates + alcohol, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7877 -0.4560 -0.0367  0.4578  2.7385 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)           5.958e+01  1.370e+01   4.348 1.40e-05 ***
## fixed_acidity         6.160e-02  1.726e-02   3.569 0.000362 ***
## volatile_acidity     -1.275e+00  7.863e-02 -16.211  < 2e-16 ***
## residual_sugar        4.318e-02  5.738e-03   7.526 6.15e-14 ***
## free_sulfur_dioxide   6.185e-03  8.390e-04   7.372 1.94e-13 ***
## total_sulfur_dioxide -2.576e-03  3.042e-04  -8.469  < 2e-16 ***
## density              -5.899e+01  1.397e+01  -4.224 2.44e-05 ***
## pH                    5.068e-01  1.004e-01   5.046 4.67e-07 ***
## sulphates             7.161e-01  8.240e-02   8.690  < 2e-16 ***
## alcohol               2.637e-01  1.951e-02  13.517  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.7339 on 5186 degrees of freedom
## Multiple R-squared:  0.2912, Adjusted R-squared:   0.29 
## F-statistic: 236.8 on 9 and 5186 DF,  p-value: < 2.2e-16
\end{verbatim}

According to the obtained model, the two variables with the highest
influence on quality are \texttt{volatile\ acidity} and
\texttt{density}, specially the latter. This can be misleading however,
as it represents the variation in quality per unit of the input, and the
scales of the input variables differ in their order of magnitude.
Checking the distribution of \(density\), for example, the difference
between the maximum and minimun values is lower than 0.03, whereas
\(total_sulfur_dioxide\) has a range of over 400. In any case, some
conclusions can be extracted from this output. For example, denser wines
tend to have a smaller perceived quality due to the negative
coefficient, and, on the contrary, those with higher alcohol contents
obtain highger quality perception.

The adjusted R-squared coefficient of the resulting model is rather low,
with a value of only 0.29. The regression overall \(p\)-value, however,
is low enough for us to be confident that there exists a relationship
between at least part of the input and the output variable, \(quality\),
performing better than just the simple constant model.

\section{Assessing the model:
assumptions}\label{assessing-the-model-assumptions}

Let us further examine the quality of the linear regression model by
checking if the assumptions are met. In particular, we are interested in
testing if the residuals are independent, normal and have constant
variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# A: Check the assumptions}
\CommentTok{# Linearity}
\KeywordTok{raintest}\NormalTok{(model1) }\CommentTok{# Yes, linear}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Rainbow test
## 
## data:  model1
## Rain = 1.0067, df1 = 2598, df2 = 2588, p-value = 0.4325
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test normality}
\KeywordTok{jarque.bera.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(model1)) }\CommentTok{# Answer: No normality}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Jarque Bera Test
## 
## data:  residuals(model1)
## X-squared = 287.77, df = 2, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Equal variances}
\KeywordTok{bptest}\NormalTok{(model1) }\CommentTok{# No constant variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  studentized Breusch-Pagan test
## 
## data:  model1
## BP = 79.793, df = 9, p-value = 1.777e-13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Testing independence of the residuals}
\KeywordTok{dwtest}\NormalTok{(model1, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\CommentTok{# Not independent}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Durbin-Watson test
## 
## data:  model1
## DW = 2.0093, p-value = 0.7377
## alternative hypothesis: true autocorrelation is not 0
\end{verbatim}

The conducted tests have different results. First of all, the Rainbow
test tests the linear relationship between the response and the linear
predictor. The \(p\)-value is high enough to not reject the null
hypothesis (0.4325) but in other datasets this value is close to 1. On
the other hand, the Durbin-Watson test returns a high \(p\)-value
(0.7377), sign that there is not enough evidence to reject the null
hypothesis of autocorrelation of the residuals. Hence, the residuals can
be considered as independent. The results for the Jarque-Bera and
Breusch-Pagan tests, however, are not so positive. According to their
results, residuals are neither normal nor homoscedastic.

\begin{figure}

{\centering \includegraphics{quality-regression_files/figure-latex/residualplot-1} 

}

\caption{Residual plot}\label{fig:residualplot}
\end{figure}

Although the residual plots are unusual due to the clustering around the
integer values, they confirm the previous results. The Normal Q-Q plot
is in line with the results of the Jarque-Bera test, showing departures
from normality, specially in the first quartiles. From the Cook's
distance plot, a significant outlier can be seen, corresponding to
observation 4446. Even if we remove this outlier, the statistical
assumptions remain the same.

\section{Evaluation of results}\label{evaluation-of-results}

The final step is to evaluate the performance of the model. In order to
do so, let us measure the accuracy of predictions by calculating the
value of our error metric of choice: RMSE. The standard deviation of our
data will be used as a reference to assess the correctness of RMSE
values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Standard deviation of our data}
\KeywordTok{sd}\NormalTok{(df$quality)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8732716
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2. Assess the model. B: Evaluate performance}
\NormalTok{predicted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model1, test)}
\NormalTok{rmse <-}\StringTok{ }\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test$quality -}\StringTok{ }\NormalTok{predicted)^}\DecValTok{2}\NormalTok{)))}
\NormalTok{rmse}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7426711
\end{verbatim}

As the results above show, our predicted variable \(quality\) has a
standard deviation of \(\sigma = 0.87\). When fed with the test set, the
linear regression model outputs predictions with an \(RMSE = 0.74\).
Therefore, and in line with what has been stated before, the
interpretation of these results is positive, as RMSE is lower than the
data's standard deviation.

\subsection{Alternative model based on the sweetness of
wines}\label{alternative-model-based-on-the-sweetness-of-wines}

We can try to improve the validation and interpretation of the model by
defininig a new categorical variable, \(residual_sugar2\), which divides
wines in three groups according on their sweetness{[}1{]}. The sweetness
of the wines is defined by its residual sugar, and commonly the
classification is \texttt{dry} wines for values up to 4 g/l,
\texttt{medium\_dry} up to 12 g/l, \texttt{medium} up to 45 g/l and
\texttt{sweet} more than 45 g/l. In out dataset, the highest value for
\texttt{residual\_sugar} is 18, thus we only consider the first three
clusters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Adding the new attribute}
\NormalTok{df$residual_sugar2 <-}\StringTok{ }\NormalTok{df$residual_sugar}
\NormalTok{df$residual_sugar2[df$residual_sugar <}\StringTok{ }\FloatTok{4.0}\NormalTok{] <-}\StringTok{ "dry"}
\NormalTok{df$residual_sugar2[df$residual_sugar >=}\StringTok{ }\FloatTok{4.0} \NormalTok{&}\StringTok{ }\NormalTok{df$residual_sugar <}\StringTok{ }\FloatTok{12.0}\NormalTok{] <-}\StringTok{ "medium dry"}
\NormalTok{df$residual_sugar2[df$residual_sugar >=}\StringTok{ }\FloatTok{12.0}\NormalTok{] <-}\StringTok{ "medium"}

\NormalTok{df$residual_sugar2 =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(df$residual_sugar2)}
\NormalTok{df$residual_sugar2=}\KeywordTok{relevel}\NormalTok{(df$residual_sugar2, }\DataTypeTok{ref=}\StringTok{"dry"}\NormalTok{)}

\CommentTok{# Update train and test sets }
\NormalTok{train =}\StringTok{ }\NormalTok{df[}\DecValTok{1}\NormalTok{:split,]}
\NormalTok{test =}\StringTok{ }\NormalTok{df[split:}\KeywordTok{nrow}\NormalTok{(df),]}

\NormalTok{model2 =}\StringTok{ }\KeywordTok{update}\NormalTok{(model1, ~.+residual_sugar2+residual_sugar:residual_sugar2)}
\KeywordTok{summary}\NormalTok{(model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = quality ~ fixed_acidity + volatile_acidity + residual_sugar + 
##     free_sulfur_dioxide + total_sulfur_dioxide + density + pH + 
##     sulphates + alcohol + residual_sugar2 + residual_sugar:residual_sugar2, 
##     data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8172 -0.4577 -0.0354  0.4526  2.6725 
## 
## Coefficients:
##                                            Estimate Std. Error t value
## (Intercept)                               6.881e+01  1.387e+01   4.962
## fixed_acidity                             6.448e-02  1.725e-02   3.738
## volatile_acidity                         -1.311e+00  7.870e-02 -16.663
## residual_sugar                            1.519e-01  2.304e-02   6.591
## free_sulfur_dioxide                       5.922e-03  8.407e-04   7.044
## total_sulfur_dioxide                     -2.612e-03  3.065e-04  -8.523
## density                                  -6.835e+01  1.413e+01  -4.837
## pH                                        5.272e-01  1.004e-01   5.253
## sulphates                                 7.053e-01  8.240e-02   8.560
## alcohol                                   2.448e-01  1.998e-02  12.249
## residual_sugar2medium                     8.122e-01  1.818e-01   4.467
## residual_sugar2medium dry                 3.532e-01  7.809e-02   4.523
## residual_sugar:residual_sugar2medium     -1.451e-01  2.459e-02  -5.900
## residual_sugar:residual_sugar2medium dry -1.265e-01  2.349e-02  -5.387
##                                          Pr(>|t|)    
## (Intercept)                              7.20e-07 ***
## fixed_acidity                            0.000187 ***
## volatile_acidity                          < 2e-16 ***
## residual_sugar                           4.80e-11 ***
## free_sulfur_dioxide                      2.11e-12 ***
## total_sulfur_dioxide                      < 2e-16 ***
## density                                  1.36e-06 ***
## pH                                       1.55e-07 ***
## sulphates                                 < 2e-16 ***
## alcohol                                   < 2e-16 ***
## residual_sugar2medium                    8.11e-06 ***
## residual_sugar2medium dry                6.24e-06 ***
## residual_sugar:residual_sugar2medium     3.87e-09 ***
## residual_sugar:residual_sugar2medium dry 7.49e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.7312 on 5182 degrees of freedom
## Multiple R-squared:  0.2969, Adjusted R-squared:  0.2952 
## F-statistic: 168.4 on 13 and 5182 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test linearity}
\KeywordTok{raintest}\NormalTok{(model2) }\CommentTok{# Yes, linear}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Rainbow test
## 
## data:  model2
## Rain = 1.0076, df1 = 2598, df2 = 2584, p-value = 0.4236
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test normality}
\KeywordTok{jarque.bera.test}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(model2)) }\CommentTok{# Answer: No normality}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Jarque Bera Test
## 
## data:  residuals(model2)
## X-squared = 289.9, df = 2, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Equal variances}
\KeywordTok{bptest}\NormalTok{(model2) }\CommentTok{# No constant variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  studentized Breusch-Pagan test
## 
## data:  model2
## BP = 93.627, df = 13, p-value = 2.818e-14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Testing independence of the residuals}
\KeywordTok{dwtest}\NormalTok{(model2, }\DataTypeTok{alternative=}\StringTok{"two.sided"}\NormalTok{) }\CommentTok{# Not independent}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Durbin-Watson test
## 
## data:  model2
## DW = 2.0065, p-value = 0.8158
## alternative hypothesis: true autocorrelation is not 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2. Assess the model. B: Evaluate performance}
\NormalTok{predicted2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model2, test)}
\NormalTok{rmse <-}\StringTok{ }\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test$quality -}\StringTok{ }\NormalTok{predicted2)^}\DecValTok{2}\NormalTok{)))}
\NormalTok{rmse}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.74352
\end{verbatim}

In comparison with the previous model, the R-squared coeffient has
improved slightly. The model's assumptions, however, remain the same:
residuals are linear, but they are not normal and their variance is
still not constant either. RMSE does not change significantly, differing
in less than 0.01 from the previous measure.

Overall, the improvement in R-squared does not justify the effort to
include a new attribute, as a) it is very slight, b) the model's
assumptions are still not met, and c) RMSE has not improved as a
consequence of it. It is true, however, that by looking at the
coefficients, some information can be derived about the impact of
sweetness on quality. Specifically, medium wines seem to have higher
quality rankings than medium-dry ones, which in turn are ranked better
than dry wines.

\section{Conclusions}\label{conclusions}

According to evaluation results, the model has a rather good performance
at predicting wine quality. Even though the it does not have a very high
R-squared coefficient, the RMSE metric is low enough for us to consider
it an interesting option to predict a wine's quality. Some remarks can
be made, however. First of all, our input data is not uniformly
distributed, with a majority of the observations taking quality values
between 4 and 6. Thus, our model could be slightly overfitted for those
kind of values, and there is a risk that the precision of results could
drop if the input data happened to contain more extreme values.

Besides, it would be interesting to try a different approach to this
problem, perhaps using a classification technique for prediction
instead, although linear regression seems better suited given the
features of the output variable.

\section{Bibliography}\label{bibliography}

{[}1{]} Terms used to indicate sweetness of wine.
\url{https://en.wikipedia.org/wiki/Sweetness_of_wine\#Residual_sugar}


\end{document}
