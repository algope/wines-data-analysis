---
title: "Logistic Regression"
header-includes:
  - \usepackage{bm}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
```

## Introduction

Oh!, a wine factory is going to receive a new pack of different wines and they do not have their type labelled (red or white). Ok, don't worry, we can go through each of the wines, look at it color, and label it. But... we would like to do this process automatically.  In the following lines, we will face a classification problem to predict if the wine is red or white, depending on its physicochemical attributes. 

A classification problem relates input variables $x$ to the output variable $y$, but now $y$ can take only discrete values, instead of continuous variables as in regression. When $y$ can only take two discrete, it is called binary classification. We will denote these values as $y \in \{0, 1\}$ in the rest of the report, where $0 \equiv$ white class and $1 \equiv$ red type.


## Loading the data

The data science pipeline often* needs to split the original dataset into two smaller pieces: the train and test datasets. If we only evaluate our models in the same dataset, the results will be overestimated (aka overfitting). To provide honest assesments of the performance of the predictive models, we will need to validate the models using a test dataset, a partition that has not been used to build the models in order to avoid bias.

(*) Some statistical learning models are robust enough to do not need this division. They can infer the behaviour of the whole population from a sample if some statistical hypothesis are fulfilled.


```{r load, message=FALSE, warning=FALSE}
# Loading the dataset into a dataframe
df <- read_delim("../data/processed/wines.csv", ";", escape_double = FALSE, trim_ws = TRUE)

# Train and test dataset, split 80%.
split = nrow(df)*0.8
train = df[1:split,]
test = df[split:nrow(df),]
```

In this case, the test dataset consists of the 20% of the dataset (1300 observations). 

# Logistic Regression Model

The equivalent linear regression model in classification is the logistic regression model. This model needs to specify a function such that $p(y=0|\bm{\tilde{X}})$ and $p(y=1|\bm{\tilde{X}})$ are both greater than 0 and sum 1. The logistic function has such properties, definining the following model:

$$ p(y|\bm{\tilde{X}},\bm{\beta}) = \frac{e^{\bm{\beta} \bm{\tilde{X}}}}{1+e^{\bm{\beta} \bm{\tilde{X}}}} $$

If $\beta_i > 0$ then increasing one unit in $x_i$ will increase the probability of a success. If $\beta_i < 0$,
then the probabilty of success decrease when increasing $x_i$. When $\beta_i = 0$ , $e^0 = 1$, so the odds do not change with $x_i$.

### Full model
We start by defining a logistic regression model with all the 11 attributes as the predictors. We do not use the `quality`, used in regression, and neither the `type`, used as the target variable:

```{r full_model}
# Logistic regression model with all the variables.
log.full=glm(type~fixed_acidity+volatile_acidity+citic_acid+residual_sugar+chlorides
             +free_sulfur_dioxide+total_sulfur_dioxide+density
             +pH+sulphates+alcohol, data=train, family=binomial)
summary(log.full)
```
At first sight, each of the coeffiecient has a marginal test which attempts the null hypothesis $H_0$: $\beta_i = 0$, after adjusting the coefficients within the model. That means, it is checked the net effect of each variable and whether should be in the model or not. All the $p$-values are small enough to reject $H_0$ (considering $\alpha = 0.05$) except for `citric_acid` (0.12) and `sulphates` (0.249). Let us discard these two variables in the further analysis:

```{r f_model}
# Logistic regression model with all the variables.
log.F=glm(type~fixed_acidity+volatile_acidity+residual_sugar+chlorides
             +free_sulfur_dioxide+total_sulfur_dioxide+density
             +pH+alcohol, data=train, family=binomial)
summary(log.F)
```
All the marginal tests are now significantly small and the overall fit of the model is high enough ($p = 1$), so we do not have evidence to reject the model in favour of the simple constant model.

```{r f_model_overall}
pchisq(log.F$deviance, log.F$df.residual, lower=F)
```

### Validation

We will define the following function to validate the correctness of the model. It basically counts the number of correctly classified observations, and it is divided by the total number of examples.

```{r accuracy}
accuracy <- function(model, train, test){
  # Train error
  train_prob=model$fitted
  train_prob=ifelse(train_prob>0.5,1,0)
  d_train = table(train_prob, train$type)
  
  # Test error
  test_prob = predict(model, newdata = test, type = "response")
  test_prob = ifelse(test_prob>0.5,1,0)
  d_test = table(test_prob, test$type)
  
  train_accuracy = sum(diag(d_train))/sum(d_train)
  test_accuracy = sum(diag(d_test))/sum(d_test)
  
  return(list("train" = train_accuracy, "test" = test_accuracy))
}
```

```{r full_model_validation}
log.F.error = accuracy(log.F, train, test)
log.F.error$train
log.F.error$test
```
Hence, with the full model we obtain an accuracy of 99% in both train and test dataset. This can be explained by the fact that the `type` of a wine is clearly defined by a combination of its chemical properties, as expected.

### Simpler Model

Though we obtain a satisfactory accuracy using almost all the variables of the dataset, we would like to find out a simpler model, where just a few attributes were used. This would lead to a more understable model, easy to interpret and efficient. For instance, we could agree that an optimal model is the one which provides, at least, a 95% of correct classification.

Let us start with the simplest model: the constant model. Since we know that the classes are a bit unbalanced, let us start with the model which sets all the labels to 1.

```{r constant}
log.B=glm(type~1, data=train, family=binomial)
accuracy(log.B, train, test)
```

A bit more than 75% of accuracy just by guessing that all the wines will be red. However, we are not using the chemical information. Let us now include one of the variables to the logistic model. Which one? The one which decreases the most the AIC. The AIC is a measure of the quality of different models, relative to each of the other models. Ideal for model selection.

The function `step` does this task for us: it chooses a model by AIC in a stepwise algorithm. We would use it in the forward direction: tt starts by the simplest constant model, and it tries to achieve the best model up to the full model, previously defined. Since we will go step by step, we will set the number of `steps` manually, to see what happens in each level.

```{r stepwise_1, warning=FALSE}
#Stepwise algorithm
step(log.B, scope=list(upper=log.F), direction="forward", step=1)
```

Looking at the output, we observe that the best attribute to build a logistic regression with just a single variable is the `total_sulfur_dioxide`. The accuracy of the logistic regression variable for both train and test datasets is 92%! So finally, `type` is almost a matter of sulfur in the liquid. This is, nevertheless, not a surprise, since the most correlated variable with respect to the `type` is also this one:


```{r corr}
log.1=glm(type~total_sulfur_dioxide, data=train, family=binomial)
accuracy(log.1, train, test)
cor(df, df$type)
```

Let us include one more variable and see what happens:
```{r stepwise_2, warning=FALSE}
#Stepwise algorithm
step(log.1, scope=list(upper=log.F), direction="forward", step=1)
```


The AIC decays the most with the inclution of `density`, reching an accuracy of 95%. Using just two variables of the dataset, we are just wrong in 58 classifications (21 should have been red, and 37 white) up to a total of 1300 observations.


```{r density}
log.2=glm(type~total_sulfur_dioxide+density, data=train, family=binomial)
accuracy(log.2, train, test)
```

We stop here, since the addition of more variables does not significantly increase the accuracy of the model. The following plot summarises the accuracy of the logistic model with respect the number of variables included in the model, following the stepwise algorithm.

```{r model_comparison, fig.align="center", fig.width=6, fig.height=2.5, echo=FALSE}

x = 0:9
y = c(0.75, 0.92, 0.95, 0.98, 0.99, 0.997, 0.997, 0.997, 0.997, 0.997)
models =  cbind.data.frame(x,y)
spline_model <- as.data.frame(spline(models$x, models$y))

plot <- ggplot(models,aes(x=x, y=y)) +  geom_line(data = spline_model, aes(x = x, y = y)) +
  geom_point() + scale_x_discrete(limit = x)

plot + labs(x="Number of variables",y="Accuracy") + 
  ggtitle("Accuracy vs. number of variables in logistic regression model") +
  theme(plot.title= element_text(hjust = 0.5))
```



## TODO

- Decir que podemos usar a) modelo mÃ¡s simple con 2 variables y 95% o b) modelo con 5 variables que tienen el mismo accuracy que el que tiene todas las variables.

- Explicar plot de dos variables.

```{r two_variables_plot, fig.align="center", fig.width=6, fig.height=4, echo=FALSE}
ggplot(train, aes(x=total_sulfur_dioxide, y=density))+geom_point(aes(colour=factor(type))) + 
  scale_color_manual(values=c("white","red")) +
  theme(panel.background = element_rect(fill = 'grey', colour = 'black'), plot.title= element_text(hjust = 0.5)) +
  ggtitle("Plot total SO2 vs. density and coloured by type.")
```

- Analisis de los coeficientes de a). De b) no hace falta, simplemente decir que es similar.

