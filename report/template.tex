\documentclass[10pt]{article}
\usepackage[OT4]{fontenc}
\newtheorem{define}{Definition}
\usepackage[utf8]{inputenc}
\usepackage{bm}
%\newcommand{\Z}{{\mathbb{Z}}}
%\usepackage{psfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx} 
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\oddsidemargin=0.15in
\evensidemargin=0.15in
\topmargin=-.5in
\textheight=9in
\textwidth=6.25in

\begin{document}
\input{preamble.tex}


\homework{number}{date, 2016}{Arminda Moreno Díaz}{Jose Luis Contreras Santos, Antonio Javier González Ferrer and Alejandro González Pérez}

\pagenumbering{gobble}
\tableofcontents

\newpage
\pagenumbering{arabic}



%%%% body goes in here %%%%
\section{Logistic Regression}

\subsection{Introduction}


\paragraph*{}
Oh!, a wine factory is going to receive a new pack of different wines and they do not have their type labelled (red or white). Ok, don't worry, we can go through each of the wines, look at it color, and label it. But... we would like to do this process automatically.  In the following lines, we will face a classification problem to predict if the wine is red or white, depending on its physicochemical attributes. 

\paragraph*{}
A classification problem relates input variables $x$ to the output variable $y$, but now $y$ can take only discrete values, instead of continuous variables as in regression. When $y$ can only take two discrete, it is called binary classification. We will denote these values as $y \in \{0, 1\}$ in the rest of the report, where $0 \equiv$ white class and $1 \equiv$ red type.

\begin{figure}[H]
	\centering
	\includegraphics[width=2.5in]{figures/wines.jpg} 
\end{figure}


\subsection{Training and test dataset}

\paragraph*{}
The data science pipeline often\footnote{Some statistical learning models are robust enough to do not need this division. They can infer the behavior of the whole population from a sample if some statistical hypothesis are fulfilled.} needs to split the original dataset into two smaller pieces: the train and test datasets. If we only evaluate our models in the same dataset, the results will be overestimated (aka overfitting). To provide honest assessments of the performance of the predictive models, we will need to validate the models using a test dataset, a partition that has not been used to build the models in order to avoid bias. 

\paragraph*{}
In this case, the test dataset consists of the 20\% of the original data (1300 observations) and the training dataset is composed of 5196 data points.

\subsection{Models}

\paragraph*{}
The equivalent linear regression model in classification is the logistic regression model. This model needs to specify a function such that $p(y=0|\bm{\tilde{X}})$ and $p(y=1|\bm{\tilde{X}})$ are both greater than 0 and sum 1. The logistic function has such properties, defining the following model:

$$ p(y|\bm{\tilde{X}},\bm{\beta}) = \frac{e^{\bm{\beta} \bm{\tilde{X}}}}{1+e^{\bm{\beta} \bm{\tilde{X}}}} $$

If $\beta_i > 0$ then increasing one unit in $x_i$ will increase the probability of a success. If $\beta_i < 0$,
then the probability of success decrease when increasing $x_i$. When $\beta_i = 0$ , $e^0 = 1$, so the odds do not change with $x_i$.

\subsubsection{Full model}

\paragraph*{}
We start by defining a logistic regression model with all the 11 attributes as the predictors. We do not use the `quality`, used in regression, and neither the `type`, used as the target variable $y$:
\begin{equation*}
\begin{aligned}	
(1) \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ logit(\hat{y}) = \beta_0 + \beta_1*fixed\_acidity + \beta_2*volatile\_acidity 
 + \beta_3*citic\_acid  \\ +\beta_4*residual\_sugar 
+ \ \beta_5*chlorides\\ + \beta_6*free\_sulfur\_dioxide  \\+ \ \beta_7*total\_sulfur\_dioxide   
 \\ + \ \beta_8*density + \beta_9*pH  \\+ \ \beta_{10}*sulphates + \beta_{11}*alcohol 	
\end{aligned}
\end{equation*}

At first sight, each of the coefficient has a marginal test which attempts the null hypothesis $H_0$: $\beta_i = 0$, after adjusting the coefficients within the model. That means, it is checked the net effect of each variable and whether should be in the model or not. All the $p$-values are small enough to reject $H_0$ (considering $\alpha = 0.05$) except for `citric\_acid` (0.12) and `sulphates` (0.249). Let us discard these two variables in the further analysis:

\begin{equation*}
\begin{aligned}	
(2) \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  logit(\hat{y}) = \beta_0 + \beta_1*fixed\_acidity + \beta_2*volatile\_acidity 
  +\beta_3*residual\_sugar \\
+ \ \beta_4*chlorides + \beta_5*free\_sulfur\_dioxide  \\+ \ \beta_6*total\_sulfur\_dioxide   
 + \ \beta_7*density \\ + \ \beta_8*pH  +  \beta_{9}*alcohol
\end{aligned}
\end{equation*}

All the marginal tests are now significantly small and the overall fit of the model is high enough ($p = 1$) testing against a Chi-Squared Distribution (pchisq), so we do not have evidence to reject the model in favor of the simple constant model.

\paragraph*{}
We have defined the following metric to validate the correctness of the model based on the confusion matrix of the model. It basically counts the number of correctly classified observations, and it is divided by the total number of examples:

\begin{center}
	accuracy = $\displaystyle \frac{TP+TN}{TP+TN+FP+FN}$
\end{center}

Hence, with the full model we obtain an accuracy of 99\% in both train and test dataset. This can be explained by the fact that the `type` of a wine is clearly defined by a combination of its chemical properties, as expected.

\subsubsection{Simpler models}

\paragraph*{}
Though we obtain a satisfactory accuracy using almost all the variables of the dataset, we would like to find out a simpler model, where just a few attributes were used. This would lead to a more understandable model, easy to interpret and efficient. For instance, we could agree that an optimal model is the one which provides, at least, a 95\% of correct classification.


\paragraph{Constant model}	

Let us start with the simplest model: the constant model. Since we know that the classes are a bit unbalanced, let us start with the model which sets all the labels to 1.

\paragraph*{}
(3) \ \ \ \  $logit(\hat{y}) = 1$

\paragraph*{}
A bit more than 75\% of accuracy just by guessing that all the wines will be red. However, we are not using the chemical information. Let us now include one of the variables to the logistic model. Which one? The one which decreases the most the AIC. The AIC is a measure of the quality of different models, relative to each of the other models. Ideal for model selection.

\paragraph*{}
The function `step` does this task for us: it chooses a model by AIC in a stepwise algorithm. We would use it in the forward direction: tt starts by the simplest constant model, and it tries to achieve the best model up to the full model, previously defined. Since we will go step by step, we will set the number of `steps` manually, to see what happens in each level.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Variable}} & \multicolumn{1}{c|}{\textbf{AIC}} \\ \hline
		total\_sulfur\_dioxide                  & 2306.6                            \\ \hline
		volatile\_acidity                       & 3522.8                            \\ \hline
		chlorides                               & 3829.2                            \\ \hline
		free\_sulfur\_dioxide                   & 4178.3                            \\ \hline
		fixed\_acidity                          & 4594.0                            \\ \hline
		residual\_sugar                         & 4925.6                            \\ \hline
		density                                 & 4929.1                            \\ \hline
		pH                                      & 5278.9                            \\ \hline
		alcohol                                 & 5836.3                            \\ \hline
	\end{tabular}
		\caption{Comparing logistic regression models using just one predictor}
		\label{table:one-predictor}
\end{table}

\paragraph*{}
Looking at the output in Table \ref{table:one-predictor}, we observe that the best attribute to build a logistic regression with just a single variable is the `total\_sulfur\_dioxide`. The accuracy of the logistic regression variable for both train and test datasets is 92\%! So finally, `type` is almost a matter of sulfur in the liquid. This is, nevertheless, not a surprise, since the most correlated variable with respect to the `type` is also this one (-0.7003).

\paragraph{Two variables model (95\% accuracy)}
Let us include one more variable to the following model and see what happens:

\paragraph*{}
(4) \ \ \ \ $logit(\hat{y}) = \beta_0 + \beta_1*total\_sulfur\_dioxide$

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Variable}} & \multicolumn{1}{c|}{\textbf{AIC}} \\ \hline
		density                                 & 1300.4                            \\ \hline
		volatile\_acidity                       & 1405.4                            \\ \hline
		chlorides                               & 1551.1                            \\ \hline
		fixed\_acidity                          & 1942.0                            \\ \hline
		alcohol                                 & 2072.2                            \\ \hline
		pH                                      & 2142.6                            \\ \hline
		residual\_sugar                         & 2279.6                            \\ \hline
		free\_sulfur\_dioxide                   & 2282.7                            \\ \hline
	\end{tabular}
	\caption{Comparing logistic regression models using two predictors, given `total\_sulfur\_dioxide`.}
	\label{table:two-predictors}
\end{table}


The AIC decays the most with the inclusion of `density`, reaching an accuracy of 95\%. Using just two variables of the dataset, we are just wrong in 58 classifications (21 should have been red, and 37 white) up to a total of 1300 observations.

\paragraph*{}
(5) \ \ \ $logit(\hat{y}) = -782.71663  -0.07262*total\_sulfur\_dioxide +792.08044  *density$

\paragraph*{}
The odds can be interpreted such that the `density` contributes to the probability of getting a red wine (success) while the `total\_sulphur\_dioxide` increases the probability of being classified as a whine type, controlling for the other variable.

\paragraph*{}
We stop here, since the addition of more variables does not significantly increase the accuracy of the model. The following plot summarizes the accuracy of the logistic model with respect the number of variables included in the model, following the stepwise algorithm.


\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{figures/accuracy-model.png} 
	\caption{Tradeoff between the number of variables used as predictors in the logistic regression and the accuracy.}
	\label{figure:accuracy-model}
\end{figure}

\paragraph{Five variables model (99\% accuracy)}
We can detect in Figure \ref{figure:accuracy-model} that using just 5 variables in the logistic regression mode we achieve the same accuracy (99\%) than using all the 9 variables from the full model (2), getting this final model:
\begin{equation*}
\begin{aligned}	
(6) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \   logit(\hat{y}) = -1.945e^{+3}  -4.375e^{-2}*total\_sulfur\_dioxide + 1.937e^{3}*density \\ \  -7.818e^{-1}*residual\_sugar \\  + \ 2.065*alcohol \\ + \ 7.016*volatile\_acidity
\end{aligned}
\end{equation*}	

Both `total\_sulphur\_dioxide` and `residual\_sugar` decreases the odds of red wine. The probability of being a red wine is less than the probability of being a white whine if you have high values of these two variables. On the other hand, `density`, `alcohol`and `volatile\_acidity` increases the odds of being red type. For instance, when `density` is increased by one unit and all other variables are held constant the odds of $y = 1$ are multiplied by $e^{3}$.

\subsection{Discussion}

\paragraph*{}
After our analysis, we have seen that two different approaches are possible in order to solve our classification problem, introduced at the beginning of this document:

\begin{itemize}
	\item \textbf{Best classification}: In this case, we would like to get the best accuracy as possible. We do not worry in terms of interpretation, but we are looking for the model with less number of parameters that reaches the best accuracy. The model (6) has a 99\% of success using 5 of the original variables. 
	\item \textbf{Simplest explanation}: We are interesting in understanding the data and interpret the parameters of the classification predictors as straightforward as possible, imposing a minimum of accuracy. In our case, the model (5) reaches a 95\% accuracy using just 2 variables, which is very useful to easily describe the model. For instance, we can plot these two variables in the plane:
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=5in]{figures/cluster.png} 
		\caption{Plot total SO$_2$ vs. density and colored by type.}
		\label{figure:cluster}
	\end{figure}
	
	This plot shows the variables `density` and `total\_sulphur\_dioxide` but colored by type. As we can see, it is clear that we have an almost perfect clusters between red and white wines. That is why we are having such a great 95\% accuracy in our simpler model. 
\end{itemize}

\paragraph*{TO (POSSIBLE DO:)}
\begin{itemize}
	\item Analisis de residuos (o comentar porque no los analizamos).
	\item Hacer plot(model) y comentar algo, especialmente del último plot.
\end{itemize}
\end{document}




